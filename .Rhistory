# DYX data from the Heart Trends group
saveit <- read.csv("dyysave.dat", header = TRUE)
twins <- read.csv("dyytwins.dat", header = TRUE)
# All DYX and time data included here
df <- rbind(saveit, twins)
# Selected variables for the DYX data
svar <- c(
"File_Name",
"Start_Date",
"Start_Time",
"Final_Dyx",
"Average_R2R"
)
# Reduce to pertinent data and fix names
df <- df[c(svar)]
df$Start_Time <- hms(str_sub(df$Start_Time, start = -8))
df$Start_Date <- dmy(df$Start_Date)
df$Average_R2R <- round(((1000 / df$Average_R2R) * 60), digits = 0)
names(df) <- c("patid", "date", "time", "DYX", "HR")
# Add a binary component to the DYX values with cutpoint of 2.0
df$CP <- 0
df$CP[df$DYX < 2.0] <- 1
# Clean the data
rm(saveit, twins, svar)
# Final DYX data and ID numbers
id_dyx <- unique(df$patid)
df_dyx <- df
# Chunk 5
## Frequency domain variable set-up
# Collect the initial start time for each patient from Holter data
starter <- df_dyx[ !duplicated(df_dyx$patid), ]
starter <- starter[c("patid", "time")]
names(starter)[names(starter) == "time"] <- "start"
starter$start <- starter$start + today()
# High frequency
hf <- read.csv("hrv_hf.dat", header = TRUE)
hf <- hf[-2]
hf <- gather(hf, timestamp, value, -patid)
hf$timestamp <- as.numeric(substring(hf$timestamp, 2))
# Low Frequency
lf <- read.csv("hrv_lf.dat", header = TRUE)
lf <- lf[-2]
lf <- gather(lf, timestamp, value, -patid)
lf$timestamp <- as.numeric(substring(lf$timestamp, 2))
# Very low frequency
vlf <- read.csv("hrv_vlf.dat", header = TRUE)
vlf <- vlf[-2]
vlf <- gather(vlf, timestamp, value, -patid)
vlf$timestamp <- as.numeric(substring(vlf$timestamp, 2))
# RR intervals
rr <- read.csv("hrv_rr.dat", header = TRUE)
rr <- rr[-2]
rr <- gather(rr, timestamp, value, -patid)
rr$timestamp <- as.numeric(substring(rr$timestamp, 2))
# Pull all PSDs into one table
psd <-
bind_rows(hf, lf, vlf, rr, .id = "psd")
psd <-
psd %>%
spread(., psd, value) %>%
merge(., starter, by = "patid")
# Correct the time stamps
psd$timestamp <- as.POSIXct(psd$start + minutes(psd$timestamp))
psd <- psd[-7]
# Reorganize data by ID and time stamp
psd <-
psd %>%
group_by(patid) %>%
arrange(timestamp, .by_group = TRUE)
names(psd)[2] <- "time"
names(psd)[3] <- "hf"
names(psd)[4] <- "lf"
names(psd)[5] <- "vlf"
names(psd)[6] <- "rr"
# Convert RR interval to heart rate
psd$BPM <- round(((1000 / psd$rr) * 60), digits = 0)
# Take log-normal of these highly variant values
psd <- within(psd, {
HF <- round(log(hf), digits = 2)
LF <- round(log(lf), digits = 2)
VLF <- round(log(vlf), digits = 2)
})
# Remove erroneous variables, e.g. negative HR, NAs, etc
psd <- na.omit(psd)
psd <- psd[ psd$BPM > 0 & psd$BPM < 200, ]
# Final HRV data set
id_frq <- unique(psd$patid)
df_frq <- psd[c("patid", "time", "HF", "LF", "VLF", "BPM")]
# Chunk 6
## Finding patients with complete data
# Removal of patients with missing DYX
id1 <- Reduce(intersect, list(id_twins, id_dyx))
n_no_DYX <- length(id_twins) - length(id1)
# Removal of missing PSD
id2 <- Reduce(intersect, list(id1, id_frq))
n_no_PSD <- length(id1) - length(id2)
# Patients with complete data
id <- Reduce(intersect, list(id_twins, id_dyx, id_frq))
df_twins <- df_twins[ which(df_twins$patid %in% id), ]
df_frq <- df_frq[ which(df_frq$patid %in% id), ]
df_dyx <- df_dyx[ which(df_dyx$patid %in% id), ]
# Chunk 7
## Data normalization
# Apply z-transfomration to DYX data
df_dyx <- within(df_dyx, {
DYX <- (DYX - mean(DYX)) / sd(DYX)
HR <- (HR - mean(HR)) / sd(HR)
})
# Apply z-transformation to frequency data
df_frq <- within(df_frq, {
HF <- (HF - mean(HF)) / sd(HF)
LF <- (LF - mean(LF)) / sd(LF)
VLF <- (VLF - mean(VLF)) / sd(VLF)
BPM <- (BPM - mean(BPM)) / sd(BPM)
})
df_frq <- df_frq[c("patid", "time", "HF", "LF", "VLF", "BPM")]
install.packages("lubridate")
sessionInfo()
# Chunk 1: setup
knitr::opts_chunk$set(
cache = TRUE,
warning = FALSE,
eval = TRUE,
echo = FALSE,
include = TRUE,
message = FALSE,
results = "asis",
dpi = 600,
dev = "png",
options("scipen" = 0, "digits" = 1)
)
# Chunk 2
# R run time
ptm <- proc.time()
## Versions
# 0. First attempt at working with the DYX data, includes ischemia and depression.
# 1. Focuses just on DYX data, has markup for how data was organized.
# 2. DYX data overnight with a set of MEAN DYX per PATIENT instead of Hourly
# 3. Focused on ischemia, Forest Plot for all 24 hours, excluding beta blockers and prior coronary disease.
# 4. Focused on depression instead, Forest Plot for all 24, excluding beta blockers and prior coronary disease
# 5. Heart rate binned (10) to look at ischemia prediction by Forest Plot
# 6. Feature development and data file creation
# 7. Reanalysis of cohort and correct selection of patients
# 8. Formal first draft of significant findings
# 9. Addition of ROC curves based on the logistic regression models
# 10. Start developing manuscript with RESULTS section first
# 11. RESULTS section being updated after meeting with Amit
# 12. RESULTS again focusing on 5PM as our most significant hour
# 13. RESULTS: exploration of data further, attempting to make a "best" classifier with splines
# 14. Focused on how 24-hour data is significant, appropriate feature table for the mixed model, showing the best cut-offs using splines, and then mixed model with appropriate random/fixed effects
# 15. Data exploration again with suggestions from February EPICORE meeting. Updated to fix issues with hours.
# 16. Incorporation of DYX and HRV together, will need time stamps. Reimported data for thoroughness.
# 17. ECGs incorporation, examination of difference between cut-points
# 18. Final approach, with normalized variable data.
# 19. Reorganization, improvement in visualizations, addition of specific analyses per Amit
# 20. Final tune up of tables and figures
# 21. Word document formatted for JAMA
## Libaries
library(rmarkdown)
library(knitr)
library(kableExtra)
library(scales)
library(ggthemes)
library(reshape2)
library(gridExtra)
library(Hmisc)
library(lme4)
library(lattice)
library(rms)
library(boot)
library(RColorBrewer)
library(stringr)
library(compareGroups)
library(ggridges)
library(FSA)
library(broom)
library(citr)
library(magrittr)
library(tidyverse)
library(lubridate)
## Plotting themes
theme_set(theme_minimal())
# Chunk 3
## Demographic twin data
# Twins studies data, THS1 and THS2, demographic information.
ths1 <- read.csv("big_tot_ths1.dat", header = TRUE)
ths2 <- read.csv("big_tot_ths2.dat", header = TRUE)
# Will need to make the data frames similar for the pair ID
colnames(ths1)[colnames(ths1) == "Pair"] <- "pair"
colnames(ths2)[colnames(ths2) == "Pair_id"] <- "pair"
# Selected variables that are the most important for our study
svar <- c(
"patid",
"pair",
"age",
"bmi",
"smoking",
"hptn",
"dm",
"chf",
"prevchd",
"med_beta_blockers",
"ptsd",
"beck_total",
"med_antidepr",
"PETdiff_2",
"FRS_T"
)
# Simplifying data based on prior variables
ths1 <- ths1[c(svar)]
ths2 <- ths2[c(svar)]
# Pair IDs need to be renamed based on THS1 and THS2 to stop overlaps.
ths2$pair <- ths2$pair + 2000
# Creating a data frame
df <- rbind(ths1, ths2)
# Add ECG start times
ecg <- read.csv("ecg-time-stamps.csv", header = TRUE)
ecg$ecg_time <- hm(ecg$ecg_time)
ecg <- na.omit(ecg)
df <- merge(df, ecg, by = "patid", all.x = TRUE)
# Organize depression data
df$sad_bin <- 0
df$sad_bin[df$beck_total >= 14] <- 1
df$sad_cat <- cut(df$beck_total,
breaks = c(-Inf, 6, 14, 20, Inf),
labels = c("bdi_I", "bdi_II", "bdi_III", "bdi_IV")
)
# Weight and obesity categorization
df$bmi_number <- round(as.numeric(df$bmi), 0)
df$bmi_cat <- cut(df$bmi_number,
breaks = c(-Inf, 18.5, 25, 30, 35, 40, Inf),
labels = c("underweight", "normal", "overweight", "obese_mild", "obese_moderate", "obese_severe")
)
# Original sample size and which variables to remove
n_initial <- length(df$patid)
# Remove patients with prior coronary disease
n_prevchd <- length(df$patid[df$prevchd != 0])
df <- df[df$prevchd == 0, ]
# Remove patients on beta blockers
df$med_beta_blockers[is.na(df$med_beta_blockers)] <- 0
n_beta_blockers <- length(df$patid[df$med_beta_blockers != 0])
df <- df[df$med_beta_blockers == 0, ]
# Remove patients without PET data
n_no_PET <- length(df$patid[is.na(df$PETdiff_2)])
df <- df[!is.na(df$PETdiff_2), ]
df$PETdiff_2 <- factor(df$PETdiff_2)
# Reselection of final variables to be used
fvar <- c(
"patid",
"pair",
"age",
"bmi_number",
"bmi_cat",
"smoking",
"hptn",
"dm",
"ptsd",
"beck_total",
"sad_bin",
"sad_cat",
"med_antidepr",
"PETdiff_2",
"FRS_T",
"ecg_time"
)
# Clean the workspace
rm(ths1, ths2)
# Final DF along with ID vectors
# ID selection
id_ecg <- unique(ecg$patid)
id_twins <- df$patid
df_twins <- df[c(fvar)]
# Chunk 4
## HeartTrends DYX variable set-up
# DYX data from the Heart Trends group
saveit <- read.csv("dyysave.dat", header = TRUE)
twins <- read.csv("dyytwins.dat", header = TRUE)
# All DYX and time data included here
df <- rbind(saveit, twins)
# Selected variables for the DYX data
svar <- c(
"File_Name",
"Start_Date",
"Start_Time",
"Final_Dyx",
"Average_R2R"
)
# Reduce to pertinent data and fix names
df <- df[c(svar)]
df$Start_Time <- hms(str_sub(df$Start_Time, start = -8))
df$Start_Date <- dmy(df$Start_Date)
df$Average_R2R <- round(((1000 / df$Average_R2R) * 60), digits = 0)
names(df) <- c("patid", "date", "time", "DYX", "HR")
# Add a binary component to the DYX values with cutpoint of 2.0
df$CP <- 0
df$CP[df$DYX < 2.0] <- 1
# Clean the data
rm(saveit, twins, svar)
# Final DYX data and ID numbers
id_dyx <- unique(df$patid)
df_dyx <- df
# Chunk 5
## Frequency domain variable set-up
# Collect the initial start time for each patient from Holter data
starter <- df_dyx[ !duplicated(df_dyx$patid), ]
starter <- starter[c("patid", "time")]
names(starter)[names(starter) == "time"] <- "start"
starter$start <- starter$start + today()
# High frequency
hf <- read.csv("hrv_hf.dat", header = TRUE)
hf <- hf[-2]
hf <- gather(hf, timestamp, value, -patid)
hf$timestamp <- as.numeric(substring(hf$timestamp, 2))
# Low Frequency
lf <- read.csv("hrv_lf.dat", header = TRUE)
lf <- lf[-2]
lf <- gather(lf, timestamp, value, -patid)
lf$timestamp <- as.numeric(substring(lf$timestamp, 2))
# Very low frequency
vlf <- read.csv("hrv_vlf.dat", header = TRUE)
vlf <- vlf[-2]
vlf <- gather(vlf, timestamp, value, -patid)
vlf$timestamp <- as.numeric(substring(vlf$timestamp, 2))
# RR intervals
rr <- read.csv("hrv_rr.dat", header = TRUE)
rr <- rr[-2]
rr <- gather(rr, timestamp, value, -patid)
rr$timestamp <- as.numeric(substring(rr$timestamp, 2))
# Pull all PSDs into one table
psd <-
bind_rows(hf, lf, vlf, rr, .id = "psd")
psd <-
psd %>%
spread(., psd, value) %>%
merge(., starter, by = "patid")
# Correct the time stamps
psd$timestamp <- as.POSIXct(psd$start + minutes(psd$timestamp))
psd <- psd[-7]
# Reorganize data by ID and time stamp
psd <-
psd %>%
group_by(patid) %>%
arrange(timestamp, .by_group = TRUE)
names(psd)[2] <- "time"
names(psd)[3] <- "hf"
names(psd)[4] <- "lf"
names(psd)[5] <- "vlf"
names(psd)[6] <- "rr"
# Convert RR interval to heart rate
psd$BPM <- round(((1000 / psd$rr) * 60), digits = 0)
# Take log-normal of these highly variant values
psd <- within(psd, {
HF <- round(log(hf), digits = 2)
LF <- round(log(lf), digits = 2)
VLF <- round(log(vlf), digits = 2)
})
# Remove erroneous variables, e.g. negative HR, NAs, etc
psd <- na.omit(psd)
psd <- psd[ psd$BPM > 0 & psd$BPM < 200, ]
# Final HRV data set
id_frq <- unique(psd$patid)
df_frq <- psd[c("patid", "time", "HF", "LF", "VLF", "BPM")]
# Chunk 6
## Finding patients with complete data
# Removal of patients with missing DYX
id1 <- Reduce(intersect, list(id_twins, id_dyx))
n_no_DYX <- length(id_twins) - length(id1)
# Removal of missing PSD
id2 <- Reduce(intersect, list(id1, id_frq))
n_no_PSD <- length(id1) - length(id2)
# Patients with complete data
id <- Reduce(intersect, list(id_twins, id_dyx, id_frq))
df_twins <- df_twins[ which(df_twins$patid %in% id), ]
df_frq <- df_frq[ which(df_frq$patid %in% id), ]
df_dyx <- df_dyx[ which(df_dyx$patid %in% id), ]
# Chunk 7
## Data normalization
# Apply z-transfomration to DYX data
df_dyx <- within(df_dyx, {
DYX <- (DYX - mean(DYX)) / sd(DYX)
HR <- (HR - mean(HR)) / sd(HR)
})
# Apply z-transformation to frequency data
df_frq <- within(df_frq, {
HF <- (HF - mean(HF)) / sd(HF)
LF <- (LF - mean(LF)) / sd(LF)
VLF <- (VLF - mean(VLF)) / sd(VLF)
BPM <- (BPM - mean(BPM)) / sd(BPM)
})
df_frq <- df_frq[c("patid", "time", "HF", "LF", "VLF", "BPM")]
## DYX data frame
df <- df_dyx
# Create hour time stamp for each DYX value
# Rounding "up" for reproducibility and reduced ambiguity
df$hour <- hour(ceiling_date(as_datetime(df$time), "hours"))
df
head(df)
str(df)
# Create hour time stamp for each DYX value
# Rounding "up" for reproducibility and reduced ambiguity
df$hour <- hour(ceiling_date(as_datetime(df$time), "hours"))
head(df)
# Chunk 1: setup
knitr::opts_chunk$set(
cache = TRUE,
warning = FALSE,
eval = TRUE,
echo = FALSE,
include = TRUE,
message = FALSE,
results = "asis",
dpi = 600,
dev = "png",
options("scipen" = 0, "digits" = 1)
)
# Chunk 2
# R run time
ptm <- proc.time()
## Versions
# 0. First attempt at working with the DYX data, includes ischemia and depression.
# 1. Focuses just on DYX data, has markup for how data was organized.
# 2. DYX data overnight with a set of MEAN DYX per PATIENT instead of Hourly
# 3. Focused on ischemia, Forest Plot for all 24 hours, excluding beta blockers and prior coronary disease.
# 4. Focused on depression instead, Forest Plot for all 24, excluding beta blockers and prior coronary disease
# 5. Heart rate binned (10) to look at ischemia prediction by Forest Plot
# 6. Feature development and data file creation
# 7. Reanalysis of cohort and correct selection of patients
# 8. Formal first draft of significant findings
# 9. Addition of ROC curves based on the logistic regression models
# 10. Start developing manuscript with RESULTS section first
# 11. RESULTS section being updated after meeting with Amit
# 12. RESULTS again focusing on 5PM as our most significant hour
# 13. RESULTS: exploration of data further, attempting to make a "best" classifier with splines
# 14. Focused on how 24-hour data is significant, appropriate feature table for the mixed model, showing the best cut-offs using splines, and then mixed model with appropriate random/fixed effects
# 15. Data exploration again with suggestions from February EPICORE meeting. Updated to fix issues with hours.
# 16. Incorporation of DYX and HRV together, will need time stamps. Reimported data for thoroughness.
# 17. ECGs incorporation, examination of difference between cut-points
# 18. Final approach, with normalized variable data.
# 19. Reorganization, improvement in visualizations, addition of specific analyses per Amit
# 20. Final tune up of tables and figures
# 21. Word document formatted for JAMA
## Libaries
library(rmarkdown)
library(knitr)
library(kableExtra)
library(scales)
library(ggthemes)
library(reshape2)
library(gridExtra)
library(Hmisc)
library(lme4)
library(lattice)
library(rms)
library(boot)
library(RColorBrewer)
library(stringr)
library(compareGroups)
library(ggridges)
library(FSA)
library(broom)
library(citr)
library(magrittr)
library(tidyverse)
library(lubridate)
## Plotting themes
theme_set(theme_minimal())
# Twins studies data, THS1 and THS2, demographic information.
ths1 <- read.csv("big_tot_ths1.dat", header = TRUE)
ths2 <- read.csv("big_tot_ths2.dat", header = TRUE)
# Will need to make the data frames similar for the pair ID
colnames(ths1)[colnames(ths1) == "Pair"] <- "pair"
colnames(ths2)[colnames(ths2) == "Pair_id"] <- "pair"
# Selected variables that are the most important for our study
svar <- c(
"patid",
"pair",
"age",
"bmi",
"smoking",
"hptn",
"dm",
"chf",
"prevchd",
"med_beta_blockers",
"ptsd",
"beck_total",
"med_antidepr",
"PETdiff_2",
"FRS_T"
)
# Simplifying data based on prior variables
ths1 <- ths1[c(svar)]
ths2 <- ths2[c(svar)]
# Pair IDs need to be renamed based on THS1 and THS2 to stop overlaps.
ths2$pair <- ths2$pair + 2000
# Creating a data frame
df <- rbind(ths1, ths2)
# Add ECG start times
ecg <- read.csv("ecg-time-stamps.csv", header = TRUE)
ecg$ecg_time <- hm(ecg$ecg_time)
ecg <- na.omit(ecg)
df <- merge(df, ecg, by = "patid", all.x = TRUE)
head(df)
str(df)
unlink('21_jama_doc_cache', recursive = TRUE)
ls()
setwd("../aric/")
getwd()
ls
ls()
rm(list=ls())
ls()
